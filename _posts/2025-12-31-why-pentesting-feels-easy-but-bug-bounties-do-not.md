---
date: 2025-12-31 00:00:00
layout: post
title: Why Pentesting Feels Easy, but Bug Bounties Don't?
subtitle: As a Pentester, How Do I Approach a Bug Bounty?
description: Why bug bounties punish the pentesting mindset more than technical gaps and as a pentester, how do I approach a bug bounty?
image: /assets/img/uploads/posts/2025/12/2025-12-31-why-pentesting-feels-easy-but-bug-bounties-do-not.png
optimized_image: /assets/img/uploads/posts/2025/12/2025-12-31-why-pentesting-feels-easy-but-bug-bounties-do-not.png
category: insight
tags:
  - bugbounty
author: xchopath
---

For the opening line.

> Companies running VRPs/Bug Bounties aren't wasting money. There's a reason their production apps aren't just lab exercises.

Do you really think companies spend money on bug bounties because their login page still has a `' OR 1=1 --` SQL injection? Obviously not.

Bug bounties exist as an extra layer, not a replacement. The main security budget is allocated to internal security first, so surface-level bugs have almost certainly been identified and addressed long before the program goes public.

Does that mean every low-hanging issue is clean? No. Things like Missing Security Headers or other mostly theoretical bugs can still exist. But are companies willing to pay for them? Absolutely not.

So, why isn't a "normal" pentesting approach 100% relevant to bug bounty? Let me break it down with a few study-cases.

## 1. Structured vs Explorative Chaos

When I first started bug bounty hunting, I approached it exactly like a pentest.

The result? Zero findings. And yes, this is coming from someone who does pentesting day-to-day.

I began with the obvious, structured approach:

- `app.target.com/login`: Checking for authentication bypasses.
- `app.target.com/transaction`: Probing for SQL Injection or XSS.
- `app.target.com/insurance` redirect to `insurance.target.com`: Seeing if there was anything worth exploring.

I spent days testing what was clearly exposed and expected to be tested, only to end up with nothing to show for it.
Not because I lacked technical skill, but because bug bounties are unfair by design.
They don't reward systematic coverage, they reward unexpected discovery.

### 1.1. Explorative Chaos Pivot

A few days later, I switched to an explorative chaos approach.

> What did I actually do? Nothing fancy. I started looking for context.

For example, I revisited the `app.target.com/insurance` endpoint.
- Why does it redirect?
- It's not just another path, it's a separate application.
- Different authentication tokens, a different trust boundary.
- Token exchange mechanism between the main app and this secondary service.

For several hours, I didn't try to inject anything. No payloads, no fuzzing, no forced exploitation attempts. **I was simply observing**. But yes, this phase felt unproductive from a pentesting point of view.

### 1.2. Gather as Much Context as Possible

The following days, I continued gathering as much context as possible.

**Did I use automation? Nope. I was manually observing**, because at this stage, I wasn't trying to inventory "their" assets. I was trying to understand how the system actually worked.

Then, one feature caught my attention, `help.target.com/account-recovery`, which allows users to recover their accounts if they lose access to their email address or phone number.

I found this feature particularly interesting because whenever an old phone number or email address (one already associated with an existing account) was submitted, **the system immediately issued a temporary token**.

### 1.3. Context, Hypothesizing, Then Abuse

After gathering all the context, the next step was to formulate a hypothesis.

> Okay, what can I do with the token issued by **help.target.com/account-recovery**?

The first thing I tried was straightforward using the temporary token directly in the **Authorization** header against `app.target.com/*` (the main application).

As expected, it didn't work. Fair enough. This path had clearly already been covered during internal pentesting.

Then I started asking myself:
- What haven't their internal pentesters tested yet?

Then, I went back to `app.target.com/insurance` and remembered that this service had its own token exchange mechanism. Unlike the main application, the insurance service didn't rely on the same authentication flow. It trusted tokens that were issued elsewhere and exchanged across services.

And yes (as expected), `insurance.target.com` was vulnerable to an **Authentication Bypass** through a weird scenario.

![authentication bypass]({{ "/assets/img/uploads/posts/2025/12/2025-12-31-authentication-bypass.png" | relative_url }})

Even before exploiting it, I already suspected this path based on the context I had gathered and the hypotheses I formed (not from random testing or blind payload spraying).

![authentication bypass]({{ "/assets/img/uploads/posts/2025/12/2025-12-31-authentication-bypass-flow.png" | relative_url }})

**Note**: This is just a mirrored PoC for illustration, nothing real is disclosed.

### 1.4. Conclusion

Imagine if I had only focused on `app.target.com/*`, `help.target.com/*`, or `insurance.target.com/*` using a strict one-by-one pentesting approach. What would I have found? **Nothing**.

The path wasn't linear, starting from `app.target.com/insurance`, then pivoting to `help.target.com/account-recovery`, back to `app.target.com/insurance`, and finally landing on `insurance.target.com`.

That's why explorative chaos matters.

### 1.5. Lesson Learned

Don't jump between targets or programs too often. Take 1-2 weeks to really understand the application. Watch how it behaves, how trust moves between services, and how features are designed to function. This is about observing, not blindly testing.

No findings doesn't mean wasted time, it means investment.

----------

## 2. Testing What Exists vs Discovering What Wasn't Meant to Exist

Maybe some of you (especially if you're still figuring things out) use ffuf or similar tools like this.

```sh
ffuf -u "https://target.com/FUZZ"
```

Congratulations! You're probably wasting your time.

### 2.1. Why Blind Enumeration Fails in Bug Bounty?

![fuzz with context]({{ "/assets/img/uploads/posts/2025/12/2025-12-31-fuzz-with-context.png" | relative_url }})

I really hate it when people say.

> "They just got lucky and found an RCE or an information disclosure from just a tool."

No, that wasn't just a tool. **They exactly knew "what" to fuzz and "where" to fuzz**. What you're seeing is the "how". What you're missing is the "why."

Tools execute ideas. They don't create them. If you think it was "just a tool," you missed the entire process.

### 2.2. Discovering What Wasnâ€™t Meant to Exist

Maybe you've seen endpoints like these before `/v2/orders/*`, `/v2/transactions/*`, `/v2/payments/*`, or `/v2/accounts/*`.

If you're familiar with **API gateways and Microservices**, you'll understand that those four endpoints often map to **separate backend services** (not a single application).

For example,
- **orders** might be mapped to **http://10.10.10.11**
- **transactions** to **http://10.10.10.22**
- **accounts** to **http://10.10.10.33** and so on.

From this, we can reasonably "assume" that **each service** may be **developed by different teams** and what's **visible on the surface** may be **just those four services**.

> The real question is "what other services exist but aren't meant to be exposed?"

**Contextual Fuzzing**

By understanding the context behind `/v2/`, the API gateway, and the underlying microservices, you can use that knowledge as a foundation for "contextual" fuzzing.

The core idea of this step is **filtering the noise** (not just filtering out 404 responses).

```
ffuf -w /usr/share/SecLists/Discovery/Web-Content/directory-list-2.3-medium.txt -c -t 100 -mc all -u "https://api.target.com/v2/FUZZ/" -X GET -fc 404 -fl 412
```

In ffuf, there are many filtering options you can use:
- `-fl` (filter responses by number of lines).
- `-fw` (filter responses by number of words).
- `-fr` (filter responses that match a regex, e.g., words like "error").

These filters help **reduce noise** and focus on responses that are more likely to **reveal something meaningful**, instead of blindly sifting through hundreds of irrelevant hits.

As a result of this,

> [400] https://api.target.com/v2/customers/

I discovered a new endpoint `/v2/customers/*`. What's fascinating is that this endpoint **wasn't part of the application's visible surface** at all.

But did I already have a finding? Of course not. **I was still getting errors**.

- But different kinds of errors, which **indicated this was a different service**.

At this stage, I refined my filtering strategy by adding **-fr '(Bad Request)'** to filter out generic Bad Request responses and I shifted one directory deeper **https://target.com/v2/customers/FUZZ**.

```sh
ffuf -w /usr/share/SecLists/Discovery/Web-Content/directory-list-2.3-medium.txt -c -t 100 -mc all -u "https://api.target.com/v2/customers/FUZZ/" -X GET -fc 400 -fr '(Bad Request)'
```

Have I already made a finding? Not yet. And the worst part is, there were zero results.

At this point, I assumed that since this was a customers endpoint, it **probably required an ID**.

Based on that assumption, I changed my strategy again by adjusting the fuzzed paths.
- https://target.com/v2/customers/FUZZ/123
- https://target.com/v2/customers/123/FUZZ

```sh
ffuf -w /usr/share/SecLists/Discovery/Web-Content/directory-list-2.3-medium.txt -c -t 100 -mc all -u "https://api.target.com/v2/customers/FUZZ/123" -X GET -fc 400 -fr '(Bad Request)'
ffuf -w /usr/share/SecLists/Discovery/Web-Content/directory-list-2.3-medium.txt -c -t 100 -mc all -u "https://api.target.com/v2/customers/123/FUZZ" -X GET -fc 400 -fr '(Bad Request)'
```

And yes, I finally got one result, an endpoint that returned a 404 error instead (it wasn't a Bad Request).
> [404] https://target.com/v2/customers/123/profile

What made it even more interesting was the error message "Customer Not Found." So I replaced 123 with my own customer ID.

And boom!

![pii disclosure]({{ "/assets/img/uploads/posts/2025/12/2025-12-31-unauthenticated-pii-disclosure.png" | relative_url }})

### 2.3. Conclusion

By understanding the context, especially whether there are microservices behind what appears to be a single application. We can **go much further than surface-level** testing. Don't get stuck on what's visible.

![fuzzing summary]({{ "/assets/img/uploads/posts/2025/12/2025-12-31-fuzzing-summary.png" | relative_url }})

We also learned that even a **404 doesn't always mean "nothing here."** Sometimes, it's a signal. So don't blindly filter out every 404 response.

### 2.4. Lesson Learned

Always build context before touching your fuzzers. Because fuzzing without context is just noise generation.

----------

## Summary

I realized that both case studies wouldn't be covered in a pentest because they require a much longer timeline. Pentests are time-boxed, while understanding and exploring all of this realistically takes an entire quarter.

It took me years to realize that a **pentesting approach isn't 100% transferable to bug bounty**. Many senior hunters may have their own methods, but this is the approach I can share in this article.

These write-ups present a mirrored PoC (not 100% reproduction) of the original issues. Hopefully, it still provides valuable takeaways, both in terms of **insight** and **technical** approach.

As a closing note,
> Bug bounty doesn't reward spectacular or amazing bugs, it rewards unexpected findings. And because it's unexpected, that's why you're needed.